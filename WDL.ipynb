{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Tutorial 9 Wide & Deep Learning.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sSEvyRdqYfPK","executionInfo":{"status":"ok","timestamp":1621342745648,"user_tz":-480,"elapsed":20683,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"9859a74e-e3f8-4900-8cc8-543f93a999f7"},"source":["from google.colab import drive \n","drive.mount('/content/drive')\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1CX15gG9Y3c2","executionInfo":{"status":"ok","timestamp":1621342745652,"user_tz":-480,"elapsed":20673,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"b80b9bdc-c3e3-4336-8697-cea255531a25"},"source":["%cd /content/drive/MyDrive/Colab Notebooks/4332proj3"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/4332proj3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P0Zyq6_6Y7jC","executionInfo":{"status":"ok","timestamp":1621342824999,"user_tz":-480,"elapsed":100008,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"dcfd133a-5e06-4d55-8821-6ac09c65ed90"},"source":["!pip install tensorflow==2.1.0"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==2.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/56/0dbdae2a3c527a119bec0d5cf441655fe030ce1daa6fa6b9542f7dbd8664/tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8MB)\n","\u001b[K     |████████████████████████████████| 421.8MB 32kB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.12.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.2.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.2)\n","Collecting tensorboard<2.2.0,>=2.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.9MB 36.5MB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.12.1)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.36.2)\n","Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n","\u001b[K     |████████████████████████████████| 450kB 34.6MB/s \n","\u001b[?25hRequirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.4.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.15.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.3.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.32.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.12.4)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.8.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.19.5)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n","\u001b[?25hCollecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.3.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (56.1.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.30.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.2.2)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.7.2)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.4.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=0864d6d844071682ce6fb993ffe272e7e70227b82397ee6394da7a86f4ba84ef\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: tensorboard, tensorflow-estimator, keras-applications, gast, tensorflow\n","  Found existing installation: tensorboard 2.4.1\n","    Uninstalling tensorboard-2.4.1:\n","      Successfully uninstalled tensorboard-2.4.1\n","  Found existing installation: tensorflow-estimator 2.4.0\n","    Uninstalling tensorflow-estimator-2.4.0:\n","      Successfully uninstalled tensorflow-estimator-2.4.0\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorflow 2.4.1\n","    Uninstalling tensorflow-2.4.1:\n","      Successfully uninstalled tensorflow-2.4.1\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DghqhAHLZCTm","executionInfo":{"status":"ok","timestamp":1621342829502,"user_tz":-480,"elapsed":104501,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"10792161-ebfd-41ea-8019-a7066edb8606"},"source":["!pip install keras==2.3.1"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting keras==2.3.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n","\r\u001b[K     |▉                               | 10kB 17.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 21.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 14.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 5.8MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (2.10.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.19.5)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.0.8)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n","Installing collected packages: keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed keras-2.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z1_HnHE1YTAb","executionInfo":{"status":"ok","timestamp":1621342831901,"user_tz":-480,"elapsed":106889,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"26f8914e-e2d5-46aa-a876-629bfe13f777"},"source":["from collections import Counter\n","from itertools import combinations\n","from math import sqrt\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #comment out this line if you want to use gpu\n","import random\n","from keras.layers import Concatenate, Dense, Dot, Dropout, Embedding, Input, Reshape\n","from keras.models import Model\n","from keras.callbacks import Callback, ModelCheckpoint\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import StandardScaler\n","import tensorflow\n","import pandas as pd\n","import ast\n","import math"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"kxzAk2g7YTAk"},"source":["### Set random seed"]},{"cell_type":"code","metadata":{"id":"hQIEjbp7YTAl","executionInfo":{"status":"ok","timestamp":1621342831908,"user_tz":-480,"elapsed":106886,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}}},"source":["import random\n","random.seed(2021)\n","np.random.seed(2021)\n","# tf > 2.0\n","tensorflow.random.set_seed(2021)\n","#tf < 2.0\n","#tf.set_random_seed(2021)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IVTVzqQOYTAl"},"source":["### Root Mean Squared Error (RMSE) is used to evaluate the performance of a recommendation algorithm, so we need to define the following utility function to compute the RMSE given the predicted ratings and the ground truth ratings. "]},{"cell_type":"code","metadata":{"id":"Gj3DppTHYTAm","executionInfo":{"status":"ok","timestamp":1621342831909,"user_tz":-480,"elapsed":106881,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}}},"source":["'''\n","params:\n","    -pred: an array containing all predicted ratings\n","    -actual: an array containing all ground truth ratings\n","    \n","return:\n","    a scalar whose value is the rmse\n","'''\n","def rmse(pred, actual):\n","    # Ignore ratings with value zero.\n","    pred = pred[actual.nonzero()].flatten()\n","    actual = actual[actual.nonzero()].flatten()\n","    return sqrt(mean_squared_error(pred, actual))"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wRURCXWnYTAm"},"source":["# Implement Wide and Deep Learning (WDL) Model"]},{"cell_type":"markdown","metadata":{"id":"OGgwqh9YYTAm"},"source":["### The wide component is a generalized linear model that takes in the raw input features and the cross-product transformation of categorical features, which enables it to learn the frequent co-occurrence of items or features. \n","\n","### The deep component is a Feed-forward Neural Network (FNN) which takes in both continuous and categorical features as input. Specifically,  the normalized values of continuous features are concatenated with the low-dimensional dense embedding vectors converted from categorical features. This concatenated vector is then fed into the FNN during each foward pass. This mechanism tend to increase the diversity of recommendations."]},{"cell_type":"code","metadata":{"id":"gCt0s8hAYTAn","executionInfo":{"status":"ok","timestamp":1621342831909,"user_tz":-480,"elapsed":106875,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}}},"source":["'''\n","params:\n","    -len_continuous: number of continuous features\n","    -deep_vocab_lens: an array of integers where deep_vocab_lens[i] represents the number of unique values of (i+1)-th deep \n","        categorical feature\n","    -len_wide: number of wide features\n","    -embed_size: dimension of the embedding vectors of deep categorical features\n","    \n","return:\n","    a keras Model object for the constructed wdl model \n","'''\n","\n","\n","def build_wdl_model(len_continuous, deep_vocab_lens, len_wide, embed_size):\n","    # A list containing all input layers\n","    input_list = []\n","    \n","    # Input layer for continuous features\n","    continuous_input = Input(shape=(len_continuous,), dtype='float32', name='continuous_input')\n","    input_list.append(continuous_input)\n","    \n","    \n","    # Get embeddings for all deep categorical features\n","    emb_list = []\n","    for vocab_size in deep_vocab_lens:\n","        _input = Input(shape=(1,), dtype='int32')\n","        input_list.append(_input)\n","        _emb = Embedding(output_dim=embed_size, input_dim=vocab_size, input_length=1)(_input)\n","        _emb = Reshape((embed_size,))(_emb)\n","        emb_list.append(_emb)\n","    \n","    \n","   \n","    # Create input layer for deep component by concatenating the embeddings and continuous features' input layer\n","    deep_input = Concatenate()(emb_list + [continuous_input])\n","    \n","\n","    # Construct deep component\n","    dense_1 = Dense(256, activation='relu')(deep_input)\n","    dense_1_dp = Dropout(0.5)(dense_1)\n","    dense_2 = Dense(128, activation='relu')(dense_1_dp)\n","    dense_2_dp = Dropout(0.2)(dense_2)\n","    dense_3 = Dense(64, activation='relu')(dense_2_dp)\n","    dense_3_dp = Dropout(0.2)(dense_3)\n","    # dense_3_dp = dense_3\n","\n","\n","    \n","    # Create input layer for wide component\n","    wide_input = Input(shape=(len_wide,), dtype='float32')\n","    input_list.append(wide_input)\n","\n","    \n","    # Concatenate the outputs of deep and wide components and feed the \n","    # concatenated vector into the finall fully connected layer\n","    fc_input = Concatenate()([dense_3_dp, wide_input])\n","    model_output = Dense(1)(fc_input)\n","    \n","    model = Model(inputs=input_list,\n","                  outputs=model_output)\n","    return model\n","    "],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gQXk0Rt3YTAp"},"source":["# Utility functions to get the values of different types of features"]},{"cell_type":"markdown","metadata":{"id":"tSWJ2Lj3YTAp"},"source":["### Continuous features"]},{"cell_type":"code","metadata":{"id":"3aU4jxuqYTAp","executionInfo":{"status":"ok","timestamp":1621342831910,"user_tz":-480,"elapsed":106870,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}}},"source":["'''\n","params:\n","    -df: input dataframe\n","    -continuous_columns: column names of continuous features\n","    \n","return: \n","    a numpy array where each row contains the values of continuous features in the corresponding row of the\n","    input dataframe\n","'''\n","def get_continuous_features(df, continuous_columns):\n","    continuous_features = df[continuous_columns].values\n","    return continuous_features"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NuBR8bFJYTAq"},"source":["### Cross product transformation of categorical features"]},{"cell_type":"code","metadata":{"id":"Po5RrlJCYTAq","executionInfo":{"status":"ok","timestamp":1621342831910,"user_tz":-480,"elapsed":106864,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}}},"source":["'''\n","params:\n","    -df: input dataframe\n","    -comb_p: number of elements in each combination (e.g., there are two elements in the combination {fried chicken, chicken and \n","    waffle}, and three elements in the combination {fried chicken, chicken and waffle, chicken fried rice})\n","    -topk: number of mostly frequent combinations to retrieve\n","    -output_freq: whether to return the frequencies of retrieved combinations\n","    \n","return:\n","    1. output_freq = True: a list X where each element is a tuple containing a combinantion tuple and corresponding frequency, and the \n","        elements are stored in the descending order of their frequencies\n","    2. output_freq = False: a list X where each element is a tuple containing a combinantion tuple, and the elements are stored in \n","    the descending order of their frequencies\n","'''\n","def get_top_k_p_combinations(df, comb_p, topk, output_freq=False):\n","    # get all combinations with comb_p\n","    def get_category_combinations(categories_str, comb_p=2):\n","        categories = categories_str.split(', ')\n","        return list(combinations(categories, comb_p))\n","    # [('Lounges', 'Dance Clubs'), ('Lounges', 'Bars'), ('Lounges', 'Nightlife'), ('Dance Clubs', 'Bars'), ('Dance Clubs', 'Nightlife'), ('Bars', 'Nightlife')]\n","    all_categories_p_combos = df[\"item_categories\"].apply(\n","        lambda x: get_category_combinations(x, comb_p)).values.tolist()\n","    # ('Lounges', 'Dance Clubs')\n","    # list of tuples that each index refer to one combination\n","    all_categories_p_combos = [tuple(t) for item in all_categories_p_combos for t in item]\n","\n","    tmp = dict(Counter(all_categories_p_combos))\n","    sorted_categories_combinations = list(sorted(tmp.items(), key=lambda x: x[1], reverse=True))\n","    if output_freq:\n","        return sorted_categories_combinations[:topk]\n","    else:\n","        return [t[0] for t in sorted_categories_combinations[:topk]]"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C3UhpKt0YTAs"},"source":["### Wide features"]},{"cell_type":"code","metadata":{"id":"LetZ0xR5YTAs","executionInfo":{"status":"ok","timestamp":1621342831911,"user_tz":-480,"elapsed":106857,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}}},"source":["'''\n","params:\n","    -df: input dataframe\n","    -selected_categories_to_idx: a dictionary mapping item categories to corrresponding integral indices\n","    -top_combinations: a list containing retrieved mostly frequent combinantions of item categories\n","    \n","return:\n","    a numpy array where each row contains the categorical features' binary encodings and cross product\n","    transformations for the corresponding row of the input dataframe\n","'''\n","\n","def get_wide_features(df, selected_categories_to_idx, top_combinations):\n","    def categories_to_binary_output(categories):\n","        binary_output = [0 for _ in range(len(selected_categories_to_idx))]\n","        for category in categories.split(', '):\n","            if category in selected_categories_to_idx:\n","                binary_output[selected_categories_to_idx[category]] = 1\n","            else:\n","                binary_output[0] = 1\n","        return binary_output\n","    def categories_cross_transformation(categories):\n","        current_category_set = set(categories.split(', '))\n","        corss_transform_output = [0 for _ in range(len(top_combinations))]\n","        for k, comb_k in enumerate(top_combinations):\n","            if len(current_category_set & comb_k) == len(comb_k):\n","                corss_transform_output[k] = 1\n","            else:\n","                corss_transform_output[k] = 0\n","        return corss_transform_output\n","\n","    category_binary_features = np.array(df.item_categories.apply(\n","        lambda x: categories_to_binary_output(x)).values.tolist())\n","    print('category_binary_features shape:',category_binary_features.shape)\n","    category_corss_transform_features = np.array(df.item_categories.apply(\n","        lambda x: categories_cross_transformation(x)).values.tolist())\n","    print('category_cross_features shape:',category_corss_transform_features.shape)\n","    out = np.concatenate((category_binary_features, category_corss_transform_features), axis=1)\n","    print('wide features shape:',out.shape)\n","    return np.concatenate((category_binary_features, category_corss_transform_features), axis=1)\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lFmgOqlNYTAt"},"source":["# Rating Prediction"]},{"cell_type":"markdown","metadata":{"id":"OknWbHFzYTAt"},"source":["### Load train, validation and test rating tables"]},{"cell_type":"code","metadata":{"id":"wkQB36LnYTAt","executionInfo":{"status":"ok","timestamp":1621342831911,"user_tz":-480,"elapsed":106851,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}}},"source":["def isnan(value):\n","    try:\n","        return math.isnan(float(value))\n","    except:\n","        return False"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"1mfPgzv2YTAt","executionInfo":{"status":"ok","timestamp":1621342832307,"user_tz":-480,"elapsed":107239,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}}},"source":["tr_df = pd.read_csv(\"data/train.csv\")\n","val_df = pd.read_csv(\"data/valid.csv\")\n","te_df = pd.read_csv(\"data/test.csv\")\n","\n","tr_ratings = tr_df.stars.values\n","val_ratings = val_df.stars.values\n","\n"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fEf83giHYTAu"},"source":["### Load content feautures tables of users and items"]},{"cell_type":"code","metadata":{"id":"pKSMIh2-YTAu","executionInfo":{"status":"ok","timestamp":1621342833820,"user_tz":-480,"elapsed":108744,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}}},"source":["user_df = pd.read_csv(\"data/user.csv\")\n","item_df = pd.read_csv(\"data/business.csv\")\n","\n","# Rename some columns of dfs and convert the indices of dfs into string type for easier reference in later stage \n","user_df = user_df.rename(index=str, columns={t: 'user_' + t for t in user_df.columns if t != 'user_id'})\n","item_df = item_df.rename(index=str, columns={t: 'item_' + t for t in item_df.columns if t != 'business_id'})\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":993},"id":"ZGXM3yttYTAu","executionInfo":{"status":"ok","timestamp":1621342833826,"user_tz":-480,"elapsed":108735,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"71490749-16fc-4536-9c3e-fc8db932cd28"},"source":["item_df"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_Unnamed: 0</th>\n","      <th>business_id</th>\n","      <th>item_name</th>\n","      <th>item_address</th>\n","      <th>item_city</th>\n","      <th>item_state</th>\n","      <th>item_postal_code</th>\n","      <th>item_latitude</th>\n","      <th>item_longitude</th>\n","      <th>item_stars</th>\n","      <th>item_review_count</th>\n","      <th>item_is_open</th>\n","      <th>item_attributes</th>\n","      <th>item_categories</th>\n","      <th>item_hours</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>de9d8b06461fa8193081e69d4e3ae345</td>\n","      <td>Apteka</td>\n","      <td>4606 Penn Ave</td>\n","      <td>Pittsburgh</td>\n","      <td>PA</td>\n","      <td>15224</td>\n","      <td>40.465694</td>\n","      <td>-79.949324</td>\n","      <td>4.5</td>\n","      <td>242</td>\n","      <td>1</td>\n","      <td>{'CoatCheck': 'False', 'BusinessParking': \"{'g...</td>\n","      <td>Nightlife, Bars, Polish, Modern European, Rest...</td>\n","      <td>{'Wednesday': '17:0-0:0', 'Thursday': '17:0-0:...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>334a85d83ec4545b5b2b4581e5182c47</td>\n","      <td>Nee House Chinese Restaurant</td>\n","      <td>13843 N Tatum Blvd, Ste 15</td>\n","      <td>Phoenix</td>\n","      <td>AZ</td>\n","      <td>85032</td>\n","      <td>33.613020</td>\n","      <td>-111.977036</td>\n","      <td>3.5</td>\n","      <td>269</td>\n","      <td>1</td>\n","      <td>{'Caters': 'True', 'GoodForKids': 'True', 'Noi...</td>\n","      <td>Chinese, Restaurants</td>\n","      <td>{'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>d39623cbcc490dff7ec1036a4f43a337</td>\n","      <td>Vintage 95</td>\n","      <td>95 W Boston</td>\n","      <td>Chandler</td>\n","      <td>AZ</td>\n","      <td>85225</td>\n","      <td>33.302093</td>\n","      <td>-111.842613</td>\n","      <td>4.0</td>\n","      <td>320</td>\n","      <td>0</td>\n","      <td>{'OutdoorSeating': 'True', 'HasTV': 'True', 'N...</td>\n","      <td>American (New), Bars, Wine Bars, Nightlife, Re...</td>\n","      <td>{'Monday': '11:0-22:0', 'Tuesday': '11:0-22:0'...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>218d525e3a7223a9e2265c2ea481116d</td>\n","      <td>Served</td>\n","      <td>1770 W Horizon Ridge, Ste 100</td>\n","      <td>Henderson</td>\n","      <td>NV</td>\n","      <td>89012</td>\n","      <td>36.010745</td>\n","      <td>-115.064803</td>\n","      <td>4.5</td>\n","      <td>664</td>\n","      <td>1</td>\n","      <td>{'DriveThru': 'False', 'RestaurantsAttire': \"'...</td>\n","      <td>Ethnic Food, American (New), Burgers, Food, Re...</td>\n","      <td>{'Monday': '0:0-0:0', 'Tuesday': '9:0-15:0', '...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>d5062bde99f4003b7b7ef546c16cf0cc</td>\n","      <td>JJ's Red Hots - Dilworth</td>\n","      <td>1514 East Blvd</td>\n","      <td>Charlotte</td>\n","      <td>NC</td>\n","      <td>28203</td>\n","      <td>35.199798</td>\n","      <td>-80.842295</td>\n","      <td>4.0</td>\n","      <td>380</td>\n","      <td>1</td>\n","      <td>{'RestaurantsReservations': 'True', 'BusinessA...</td>\n","      <td>Caterers, Hot Dogs, Restaurants, Vegetarian, A...</td>\n","      <td>{'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5959</th>\n","      <td>5959</td>\n","      <td>bfa5e2aab6f787e190082cf76e748203</td>\n","      <td>Naked City Pizza Shop</td>\n","      <td>3240 S Arville St</td>\n","      <td>Las Vegas</td>\n","      <td>NV</td>\n","      <td>89102</td>\n","      <td>36.130804</td>\n","      <td>-115.198971</td>\n","      <td>4.0</td>\n","      <td>576</td>\n","      <td>1</td>\n","      <td>{'BusinessParking': \"{'garage': False, 'street...</td>\n","      <td>Sandwiches, Pizza, Chicken Wings, Italian, Res...</td>\n","      <td>{'Monday': '12:0-21:0', 'Tuesday': '12:0-21:0'...</td>\n","    </tr>\n","    <tr>\n","      <th>5960</th>\n","      <td>5960</td>\n","      <td>bf46e96044d23cccab27cf4534af2add</td>\n","      <td>Pomo Pizzeria Gilbert</td>\n","      <td>366 N Gilbert Rd, Ste 106</td>\n","      <td>Gilbert</td>\n","      <td>AZ</td>\n","      <td>85234</td>\n","      <td>33.356542</td>\n","      <td>-111.790019</td>\n","      <td>4.0</td>\n","      <td>337</td>\n","      <td>1</td>\n","      <td>{'WheelchairAccessible': 'True', 'WiFi': \"u'fr...</td>\n","      <td>Restaurants, Italian, Pizza</td>\n","      <td>{'Monday': '0:0-0:0', 'Tuesday': '11:0-22:0', ...</td>\n","    </tr>\n","    <tr>\n","      <th>5961</th>\n","      <td>5961</td>\n","      <td>921ae754074c121b7aa9523ca7e8b7cd</td>\n","      <td>Greens and Proteins</td>\n","      <td>8975 S Eastern Ave, Ste 3C</td>\n","      <td>Las Vegas</td>\n","      <td>NV</td>\n","      <td>89123</td>\n","      <td>36.026212</td>\n","      <td>-115.119341</td>\n","      <td>4.0</td>\n","      <td>704</td>\n","      <td>1</td>\n","      <td>{'Alcohol': \"u'none'\", 'BusinessParking': \"{'g...</td>\n","      <td>Sandwiches, Cafes, Pizza, Vegetarian, Gluten-F...</td>\n","      <td>{'Monday': '9:30-19:30', 'Tuesday': '7:0-22:0'...</td>\n","    </tr>\n","    <tr>\n","      <th>5962</th>\n","      <td>5962</td>\n","      <td>8e350f4ad2441016656c1a5bcf023190</td>\n","      <td>Jessie Rae's BBQ</td>\n","      <td>5611 S Valley View Blvd</td>\n","      <td>Las Vegas</td>\n","      <td>NV</td>\n","      <td>89118</td>\n","      <td>36.087895</td>\n","      <td>-115.190329</td>\n","      <td>4.5</td>\n","      <td>595</td>\n","      <td>1</td>\n","      <td>{'Caters': 'True', 'RestaurantsTableService': ...</td>\n","      <td>Farmers Market, Caterers, Food, Street Vendors...</td>\n","      <td>{'Monday': '0:0-0:0', 'Tuesday': '11:30-20:30'...</td>\n","    </tr>\n","    <tr>\n","      <th>5963</th>\n","      <td>5963</td>\n","      <td>af0def644bc2376ec11448798fe7baa7</td>\n","      <td>JW Marriott Phoenix Desert Ridge Resort &amp; Spa</td>\n","      <td>5350 E Marriott Dr</td>\n","      <td>Phoenix</td>\n","      <td>AZ</td>\n","      <td>85054</td>\n","      <td>33.683910</td>\n","      <td>-111.966235</td>\n","      <td>4.0</td>\n","      <td>441</td>\n","      <td>1</td>\n","      <td>{'BusinessAcceptsBitcoin': 'False', 'GoodForKi...</td>\n","      <td>Golf, Hotels, Day Spas, Hotels &amp; Travel, Event...</td>\n","      <td>{'Monday': '0:0-0:0', 'Tuesday': '0:0-0:0', 'W...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5964 rows × 15 columns</p>\n","</div>"],"text/plain":["      item_Unnamed: 0  ...                                         item_hours\n","0                   0  ...  {'Wednesday': '17:0-0:0', 'Thursday': '17:0-0:...\n","1                   1  ...  {'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...\n","2                   2  ...  {'Monday': '11:0-22:0', 'Tuesday': '11:0-22:0'...\n","3                   3  ...  {'Monday': '0:0-0:0', 'Tuesday': '9:0-15:0', '...\n","4                   4  ...  {'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...\n","...               ...  ...                                                ...\n","5959             5959  ...  {'Monday': '12:0-21:0', 'Tuesday': '12:0-21:0'...\n","5960             5960  ...  {'Monday': '0:0-0:0', 'Tuesday': '11:0-22:0', ...\n","5961             5961  ...  {'Monday': '9:30-19:30', 'Tuesday': '7:0-22:0'...\n","5962             5962  ...  {'Monday': '0:0-0:0', 'Tuesday': '11:30-20:30'...\n","5963             5963  ...  {'Monday': '0:0-0:0', 'Tuesday': '0:0-0:0', 'W...\n","\n","[5964 rows x 15 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_gAzkjVEYTAv","executionInfo":{"status":"ok","timestamp":1621342835729,"user_tz":-480,"elapsed":110622,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"4f2bec11-e3b6-4334-b439-1c56079094d9"},"source":["for i,dict_str in enumerate(item_df[\"item_attributes\"]):\n","    # check for nan\n","    if(isnan(dict_str)):\n","        print(f'line {i+1} has nan index')\n","#         print('**'*20)\n","#         print(dict_str)\n","    else:\n","        dict_str = dict_str.replace('\"','')\n","        temp_dict = ast.literal_eval(dict_str)  \n","        item_df[\"item_attributes\"][i] = temp_dict"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  # Remove the CWD from sys.path while we load stuff.\n"],"name":"stderr"},{"output_type":"stream","text":["line 44 has nan index\n","line 177 has nan index\n","line 199 has nan index\n","line 335 has nan index\n","line 385 has nan index\n","line 401 has nan index\n","line 404 has nan index\n","line 488 has nan index\n","line 550 has nan index\n","line 624 has nan index\n","line 720 has nan index\n","line 770 has nan index\n","line 1065 has nan index\n","line 1112 has nan index\n","line 1130 has nan index\n","line 1134 has nan index\n","line 1191 has nan index\n","line 1384 has nan index\n","line 1413 has nan index\n","line 1506 has nan index\n","line 1510 has nan index\n","line 1775 has nan index\n","line 1883 has nan index\n","line 1885 has nan index\n","line 2024 has nan index\n","line 2349 has nan index\n","line 2375 has nan index\n","line 2415 has nan index\n","line 2429 has nan index\n","line 2476 has nan index\n","line 2506 has nan index\n","line 2601 has nan index\n","line 2802 has nan index\n","line 2823 has nan index\n","line 3232 has nan index\n","line 3269 has nan index\n","line 3356 has nan index\n","line 3378 has nan index\n","line 3549 has nan index\n","line 3714 has nan index\n","line 4086 has nan index\n","line 4155 has nan index\n","line 4290 has nan index\n","line 4651 has nan index\n","line 4777 has nan index\n","line 4785 has nan index\n","line 4851 has nan index\n","line 4907 has nan index\n","line 5565 has nan index\n","line 5810 has nan index\n","line 5846 has nan index\n","line 5902 has nan index\n","line 5909 has nan index\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yx-4XeVcYTAw"},"source":["### Associate each row in the rating tables with corresponding user's and item's content features through merging the rating tables and content features tables"]},{"cell_type":"code","metadata":{"id":"P_Jl-CBRYTAw","executionInfo":{"status":"ok","timestamp":1621342835730,"user_tz":-480,"elapsed":110612,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}}},"source":["# Save the original row indices of each rating table\n","tr_df[\"index\"] = tr_df.index\n","val_df[\"index\"]  = val_df.index\n","te_df[\"index\"] = te_df.index\n","\n","tr_df = pd.merge(pd.merge(tr_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n","val_df = pd.merge(pd.merge(val_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n","te_df = pd.merge(pd.merge(te_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IcPOzzfrYTAw"},"source":["### Prepare continuous features"]},{"cell_type":"code","metadata":{"id":"RRPuqIF2YTAw","executionInfo":{"status":"ok","timestamp":1621342836267,"user_tz":-480,"elapsed":111140,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}}},"source":["# Specify the columns containing conitnuous features\n","continuous_columns = [\"user_average_stars\", \"user_cool\", \"user_fans\", \n","                      \"user_review_count\", \"user_useful\", \"user_funny\",\n","                      \"item_is_open\", \"item_latitude\", \"item_longitude\", \n","                      \"item_review_count\", \"item_stars\"]\n","\n","# Get values of continous features for train/validation/test sets using the utility function defined previously\n","\n","tr_continuous_features = get_continuous_features(tr_df, continuous_columns)\n","val_continuous_features = get_continuous_features(val_df, continuous_columns)\n","te_continuous_features = get_continuous_features(te_df, continuous_columns)\n","\n","# Standardize each feature by removing the mean of the training samples and scaling to unit variance.\n","# See https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html for more details.\n","scaler = StandardScaler().fit(tr_continuous_features)\n","\n","tr_continuous_features = scaler.transform(tr_continuous_features)\n","val_continuous_features = scaler.transform(val_continuous_features)\n","te_continuous_features = scaler.transform(te_continuous_features)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ec8c8RnXYTAx"},"source":["### Prepare deep categorical features"]},{"cell_type":"code","metadata":{"id":"eFSMrFr7YTAx","executionInfo":{"status":"ok","timestamp":1621342836267,"user_tz":-480,"elapsed":111132,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}}},"source":["# Sepcify column names of deep categorical features\n","item_deep_columns = [\"item_city\", \"item_postal_code\", \"item_state\"]\n","\n","# An array of integers where deep_vocab_lens[i] represents the number of unique values of (i+1)-th deep categorical feature\n","item_deep_vocab_lens = []\n","\n","for col_name in item_deep_columns:\n","    # Get all unique values of this deep categorical feature\n","    tmp = item_df[col_name].unique()\n","    \n","    # Create a dictionary mapping each unique value to a unique integral index\n","    vocab = dict(zip(tmp, range(1, len(tmp) + 1)))\n","    \n","    # Get the number of unique values of this deep categorical features\n","    item_deep_vocab_lens.append(len(vocab) + 1)\n","    \n","    # Create a new column where each entry stores the integral index of this deep categorical feature's value in the same row\n","    item_df[col_name + \"_idx\"] = item_df[col_name].apply(lambda x: vocab[x])\n","\n","\n","# Create a dictionary mapping each business id to corresponding values of deep categorical features\n","item_deep_idx_columns = [t + \"_idx\" for t in item_deep_columns]\n","item_to_deep_categorical_features = dict(zip(item_df.business_id.values, item_df[item_deep_idx_columns].values.tolist()))\n","\n","# Creat numpy arrays storing corresponding deep categorical features' values of train/validation/test sets using the above mapping\n","tr_deep_categorical_features = np.array(tr_df.business_id.apply(lambda x: item_to_deep_categorical_features[x]).values.tolist())\n","val_deep_categorical_features = np.array(val_df.business_id.apply(lambda x: item_to_deep_categorical_features[x]).values.tolist())\n","te_deep_categorical_features = np.array(te_df.business_id.apply(lambda x: item_to_deep_categorical_features[x]).values.tolist())\n"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-U6BLNCYTAx"},"source":["### Prepare wide features"]},{"cell_type":"markdown","metadata":{"id":"C0wmszHiYTAx"},"source":["##### Prepare binary encoding for each selected category"]},{"cell_type":"code","metadata":{"id":"kygoweq7YTAx","executionInfo":{"status":"ok","timestamp":1621342836269,"user_tz":-480,"elapsed":111127,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}}},"source":["# Collect the categories of all items \n","all_categories = [category for category_list in item_df.item_categories.values for category in category_list.split(\", \")]\n","\n","# Sort all unique values of the item categories by their frequencies in descending order\n","category_sorted = sorted(Counter(all_categories).items(), key=lambda x: x[1], reverse=True)\n","\n","# Select top 500 most frequent categories\n","selected_categories = [t[0] for t in category_sorted[:500]]\n","\n","# Create a dictionary mapping each secleted category to a unique integral index\n","selected_categories_to_idx = dict(zip(selected_categories, range(1, len(selected_categories) + 1)))\n","\n","# Map all categories unseen in the item df to index 0\n","selected_categories_to_idx['unk'] = 0\n","\n","# Create a dictionary mapping each integral index to corresponding category\n","idx_to_selected_categories = {val: key for key, val in selected_categories_to_idx.items()}\n"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_iAYpuViYTAy"},"source":["##### Prepare cross product transformation for categories"]},{"cell_type":"code","metadata":{"id":"l0pMtjM-YTAy","executionInfo":{"status":"ok","timestamp":1621342841787,"user_tz":-480,"elapsed":116638,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}}},"source":["# Get most frequent categories combinantions using the utility function defined previously and store them in the folloing list\n","top_combinations = []\n","\n","# Get top 50 most frequent two-categories combinantions in the train set\n","\n","top_combinations += get_top_k_p_combinations(tr_df, 2, 50, output_freq=False)\n","\n","# Get top 30 most frequent three-categories combinantions in the train set\n","top_combinations += get_top_k_p_combinations(tr_df, 3, 30, output_freq=False)\n","\n","# Get top 20 most frequent four-categories combinantions in the train set\n","top_combinations += get_top_k_p_combinations(tr_df, 4, 20, output_freq=False)\n","\n","# Convert each combinantion in the list to a set data structure\n","top_combinations = [set(t) for t in top_combinations]"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IkUbLEDPYTAy","executionInfo":{"status":"ok","timestamp":1621342850734,"user_tz":-480,"elapsed":125577,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"d7c6dc42-7894-4368-e23d-2b8e9b0c9baa"},"source":["# Get values of wide features for train/validation/test sets using the utility function defined previously\n","\n","tr_wide_features = get_wide_features(tr_df, selected_categories_to_idx, top_combinations)\n","val_wide_features = get_wide_features(val_df, selected_categories_to_idx, top_combinations)\n","te_wide_features = get_wide_features(te_df, selected_categories_to_idx, top_combinations)\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["category_binary_features shape: (60080, 501)\n","category_cross_features shape: (60080, 100)\n","wide features shape: (60080, 601)\n","category_binary_features shape: (7510, 501)\n","category_cross_features shape: (7510, 100)\n","wide features shape: (7510, 601)\n","category_binary_features shape: (7510, 501)\n","category_cross_features shape: (7510, 100)\n","wide features shape: (7510, 601)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H0N5qZ9JYTAy"},"source":["### Build the input list for each of the train/validation/test sets through aggregating all continuous, deep categorical and wide features\n"]},{"cell_type":"code","metadata":{"id":"Bt3so3d1YTAz","executionInfo":{"status":"ok","timestamp":1621342852257,"user_tz":-480,"elapsed":127092,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}}},"source":["# tr_features = [tr_continuous_features,categorical_features_0,categorical_features_1,categorical_features_2,tr_wide_features]\n","tr_features = []\n","tr_features.append(tr_continuous_features.tolist())\n","tr_features += [tr_deep_categorical_features[:,i].tolist() for i in range(tr_deep_categorical_features.shape[1])]\n","tr_features.append(tr_wide_features.tolist())\n","\n","\n","\n","val_features = []\n","val_features.append(val_continuous_features.tolist())\n","val_features += [val_deep_categorical_features[:,i].tolist() for i in range(val_deep_categorical_features.shape[1])]\n","val_features.append(val_wide_features.tolist())\n","\n","te_features = []\n","te_features.append(te_continuous_features.tolist())\n","te_features += [te_deep_categorical_features[:,i].tolist() for i in range(te_deep_categorical_features.shape[1])]\n","te_features.append(te_wide_features.tolist())"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SKd6g9_fYTAz","executionInfo":{"status":"ok","timestamp":1621342852266,"user_tz":-480,"elapsed":127091,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"5cc9a6d9-62d5-49c3-85ab-f860651dbb71"},"source":["len(te_features[0])"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7510"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"ucL3t4azYTAz"},"source":["### Build the WDL model defined above"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v3uUW5TyYTAz","executionInfo":{"status":"ok","timestamp":1621342852667,"user_tz":-480,"elapsed":127482,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"64db96d2-6b41-4858-b3a2-ba4663e56ea5"},"source":["wdl_model = build_wdl_model(\n","        len(tr_continuous_features[0]),\n","        item_deep_vocab_lens,   # num of category classes\n","        len(tr_wide_features[0]), \n","        embed_size=100)\n","print(len(tr_continuous_features[0]))\n","print(item_deep_vocab_lens)\n","print(len(tr_wide_features[0]))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["11\n","[118, 799, 14]\n","601\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UxaCUr-6YTA0"},"source":["### Train the model using Adagrad optimizer and mean squared error loss"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rN713PU1YTA0","executionInfo":{"status":"ok","timestamp":1621342899359,"user_tz":-480,"elapsed":174161,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"9c31865d-9e5f-4bfa-c0a5-a317922c6440"},"source":["wdl_model.compile(optimizer='SGD', loss='mse')\n","\n","\n","history = wdl_model.fit(\n","    tr_features, \n","    tr_ratings, \n","        epochs=5, verbose=1, callbacks=[ModelCheckpoint('model.h5')])"],"execution_count":26,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/5\n","60080/60080 [==============================] - 9s 145us/step - loss: 1.2027\n","Epoch 2/5\n","60080/60080 [==============================] - 8s 141us/step - loss: 1.0574\n","Epoch 3/5\n","60080/60080 [==============================] - 8s 140us/step - loss: 1.0419\n","Epoch 4/5\n","60080/60080 [==============================] - 8s 138us/step - loss: 1.0322\n","Epoch 5/5\n","60080/60080 [==============================] - 9s 142us/step - loss: 1.0287\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GR_s6aFBYTA0"},"source":["### Evaluate the model on train and validation sets using RMSE¶"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6lMJm7wVYTA0","executionInfo":{"status":"ok","timestamp":1621342904978,"user_tz":-480,"elapsed":179769,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"497838fe-9588-4619-b7f0-0215938e5109"},"source":["y_pred = wdl_model.predict(tr_features)\n","print(\"TRAIN RMSE: \", rmse(y_pred, tr_ratings))\n","y_pred = wdl_model.predict(val_features)\n","print(\"VALID RMSE: \", rmse(y_pred, val_ratings))\n"],"execution_count":27,"outputs":[{"output_type":"stream","text":["TRAIN RMSE:  1.004384287124796\n","VALID RMSE:  0.9978442084793959\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L42aObQ9h_mu","executionInfo":{"status":"ok","timestamp":1621342905406,"user_tz":-480,"elapsed":180186,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"9e6b9c9d-069c-4254-a125-f0e358d6a11c"},"source":["res_df_val = pd.read_csv(\"data/valid.csv\")\n","res_df_val\n","res_df_val['stars'] = y_pred[:, 0]\n","NAME = 'val_pre'\n","res_df_val.to_csv(\"{}.csv\".format(NAME), index=False)\n","print(\"Writing val predictions to file done.\")"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Writing val predictions to file done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"oCGYnfBvYTA1","executionInfo":{"status":"ok","timestamp":1621342905790,"user_tz":-480,"elapsed":180559,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"9b17f00c-280c-4914-a159-269f429afddc"},"source":["y_pred = wdl_model.predict(te_features)\n","res_df = pd.read_csv(\"data/test.csv\")\n","res_df"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>business_id</th>\n","      <th>stars</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>39b627ea7d06c70fd36d5bfc6c23ddbb</td>\n","      <td>d599f2a9f7e782f0a3fc16fd5dc22027</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2fd27ce10e62ba2578d0c04799027a6e</td>\n","      <td>69423d084bc0b22ea07aae598acc8c52</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>d7995d511b5493a4b6ec34af6290e080</td>\n","      <td>9ef9cfddc1e6fab536d4517e204064be</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>36d8bb0193e3c785a2c47dba1abab521</td>\n","      <td>4380c5fcdedd19169190eaae7de83986</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>b4a3aea52dea7ad3fc324a8c8d4356fc</td>\n","      <td>88cd731bf93dd4706e824d0663018808</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7505</th>\n","      <td>42326317a1c3456c7ade772762f41d19</td>\n","      <td>e4cde8be05f410c5f29dfe0cb76b21b5</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>7506</th>\n","      <td>a557347314e8002c6ae452071e6b7e18</td>\n","      <td>a074be3d6a7f48fd17d37aa7acfe10f1</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>7507</th>\n","      <td>59c52c7a858503d1bb970923e08d1b0c</td>\n","      <td>65399b606015d6148f10f927dd4ae0ff</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>7508</th>\n","      <td>f0fa7ac5cd64443f3cfe4b2042e4a830</td>\n","      <td>94bb8df33b17a8e7678ec8d646950148</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>7509</th>\n","      <td>39045d24bb164603ceab3a084ed6c926</td>\n","      <td>dbbc0b3fd3a03fee057e2d9fd797c29f</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7510 rows × 3 columns</p>\n","</div>"],"text/plain":["                               user_id                       business_id  stars\n","0     39b627ea7d06c70fd36d5bfc6c23ddbb  d599f2a9f7e782f0a3fc16fd5dc22027    0.0\n","1     2fd27ce10e62ba2578d0c04799027a6e  69423d084bc0b22ea07aae598acc8c52    0.0\n","2     d7995d511b5493a4b6ec34af6290e080  9ef9cfddc1e6fab536d4517e204064be    0.0\n","3     36d8bb0193e3c785a2c47dba1abab521  4380c5fcdedd19169190eaae7de83986    0.0\n","4     b4a3aea52dea7ad3fc324a8c8d4356fc  88cd731bf93dd4706e824d0663018808    0.0\n","...                                ...                               ...    ...\n","7505  42326317a1c3456c7ade772762f41d19  e4cde8be05f410c5f29dfe0cb76b21b5    0.0\n","7506  a557347314e8002c6ae452071e6b7e18  a074be3d6a7f48fd17d37aa7acfe10f1    0.0\n","7507  59c52c7a858503d1bb970923e08d1b0c  65399b606015d6148f10f927dd4ae0ff    0.0\n","7508  f0fa7ac5cd64443f3cfe4b2042e4a830  94bb8df33b17a8e7678ec8d646950148    0.0\n","7509  39045d24bb164603ceab3a084ed6c926  dbbc0b3fd3a03fee057e2d9fd797c29f    0.0\n","\n","[7510 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKugtlZOenwC","executionInfo":{"status":"ok","timestamp":1621342905792,"user_tz":-480,"elapsed":180554,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"671b8843-8087-4895-93c0-47ec22003294"},"source":["res_df['stars'] = y_pred[:, 0]\n","NAME = 'test'\n","res_df.to_csv(\"{}.csv\".format(NAME), index=False)\n","print(\"Writing test predictions to file done.\")"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Writing test predictions to file done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3qjyw0lukdDk","executionInfo":{"status":"ok","timestamp":1621342939668,"user_tz":-480,"elapsed":551,"user":{"displayName":"张伟","photoUrl":"","userId":"13128747320571869044"}},"outputId":"7eea820f-5b56-4260-87a5-e240591e836e","colab":{"base_uri":"https://localhost:8080/","height":402}},"source":["res_df"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>business_id</th>\n","      <th>stars</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>39b627ea7d06c70fd36d5bfc6c23ddbb</td>\n","      <td>d599f2a9f7e782f0a3fc16fd5dc22027</td>\n","      <td>4.178943</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2fd27ce10e62ba2578d0c04799027a6e</td>\n","      <td>69423d084bc0b22ea07aae598acc8c52</td>\n","      <td>3.682924</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>d7995d511b5493a4b6ec34af6290e080</td>\n","      <td>9ef9cfddc1e6fab536d4517e204064be</td>\n","      <td>4.011315</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>36d8bb0193e3c785a2c47dba1abab521</td>\n","      <td>4380c5fcdedd19169190eaae7de83986</td>\n","      <td>4.650148</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>b4a3aea52dea7ad3fc324a8c8d4356fc</td>\n","      <td>88cd731bf93dd4706e824d0663018808</td>\n","      <td>3.672891</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7505</th>\n","      <td>42326317a1c3456c7ade772762f41d19</td>\n","      <td>e4cde8be05f410c5f29dfe0cb76b21b5</td>\n","      <td>3.831137</td>\n","    </tr>\n","    <tr>\n","      <th>7506</th>\n","      <td>a557347314e8002c6ae452071e6b7e18</td>\n","      <td>a074be3d6a7f48fd17d37aa7acfe10f1</td>\n","      <td>3.662682</td>\n","    </tr>\n","    <tr>\n","      <th>7507</th>\n","      <td>59c52c7a858503d1bb970923e08d1b0c</td>\n","      <td>65399b606015d6148f10f927dd4ae0ff</td>\n","      <td>4.082367</td>\n","    </tr>\n","    <tr>\n","      <th>7508</th>\n","      <td>f0fa7ac5cd64443f3cfe4b2042e4a830</td>\n","      <td>94bb8df33b17a8e7678ec8d646950148</td>\n","      <td>4.221411</td>\n","    </tr>\n","    <tr>\n","      <th>7509</th>\n","      <td>39045d24bb164603ceab3a084ed6c926</td>\n","      <td>dbbc0b3fd3a03fee057e2d9fd797c29f</td>\n","      <td>3.692065</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7510 rows × 3 columns</p>\n","</div>"],"text/plain":["                               user_id  ...     stars\n","0     39b627ea7d06c70fd36d5bfc6c23ddbb  ...  4.178943\n","1     2fd27ce10e62ba2578d0c04799027a6e  ...  3.682924\n","2     d7995d511b5493a4b6ec34af6290e080  ...  4.011315\n","3     36d8bb0193e3c785a2c47dba1abab521  ...  4.650148\n","4     b4a3aea52dea7ad3fc324a8c8d4356fc  ...  3.672891\n","...                                ...  ...       ...\n","7505  42326317a1c3456c7ade772762f41d19  ...  3.831137\n","7506  a557347314e8002c6ae452071e6b7e18  ...  3.662682\n","7507  59c52c7a858503d1bb970923e08d1b0c  ...  4.082367\n","7508  f0fa7ac5cd64443f3cfe4b2042e4a830  ...  4.221411\n","7509  39045d24bb164603ceab3a084ed6c926  ...  3.692065\n","\n","[7510 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"IGBQ-KdPkeez"},"source":[""],"execution_count":null,"outputs":[]}]}